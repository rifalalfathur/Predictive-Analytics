# -*- coding: utf-8 -*-
"""Proyek Pertama : Predictive Analytics

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iF56H2x6WG28EhVpaTdTTRZjwlsESB_v

M. Rifal Alfathur Fauzan
rifalalfathur@gmail.com
Kab. Bandung

# Data Loading

inisialisai library apa saja yang akan digunakan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""penggunaan dataset ini bertujuan untuk mengetahui karakteristik apa saja yang mempengaruhi harga mobil di Polandia. data set dapat didapat di https://www.kaggle.com/datasets/aleksandrglotov/car-prices-poland"""

data = pd.read_csv('/content/Car_Prices_Poland_Kaggle.csv')
data

"""terdapat 117927 baris dan 11 kolom pada dataset ini. Dan dapat dilihat terdapat variabel yang dapat dihapus yaitu "Unnamed: 0" dan ini tidak berpengaruh terhadap analysis. Untuk menghapus variabel tsb dapat menggunakan syntax berikut:"""

data.drop(['Unnamed: 0'], inplace=True, axis=1)
data.head()

"""* mark = car mark
* model = car model
* generation_name = Formatted Generation
* year = Car Year
* mileage = Car Mileage in Kilometers
* vol_engine = Auto Engine Size
* fuel = Engine Type
* city = locality in Poland
* province = Region of Poland
* price = Price in PLN (approx. 1USD=1PLN)

# **Exploratory Data Analysis - Deskripsi Variabel**

kemudian cek informasi yang terdapat pada dataset
"""

data.info()

"""*   terdapat empat kolom int64: year, mileage, vol_engine, dan price
*   terdapat enam kolom object: mark, model, generation_name, fuel, city, dan province

kemudian cek deskripsi statistik dataset
"""

data.describe()

"""*   Count  adalah jumlah sampel pada data.
*   Mean adalah nilai rata-rata.
*   Std adalah standar deviasi.
*   Min yaitu nilai minimum setiap kolom. 
*   25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas 
*   interval dalam empat bagian sebaran yang sama.
*   50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah).
*   75% adalah kuartil ketiga.
*   Max adalah nilai maksimum.

# **Menangani Missing Value**

terdapat missing value di vol_engine karena volume engine tidak mungkin 0 tidak akan 0 sehingga dapat dihilangkan, dan sama untuk mileage karena bahkan untuk setiap mobil baru pun tidak akan 0.
"""

mileage	= (data.mileage	 == 0).sum() 	
vol_engine	 = (data.vol_engine	 == 0).sum()
print("Nilai 0 di kolom mileage ada: ", mileage)
print("Nilai 0 di kolom vol_engine ada: ", vol_engine)

"""karena nilai 0 di kolom vol_engine yang paling banyak, maka disini dicek apakah nilai 0 tsb terdapat di kolom yang lain atau tidak. Dengan menggunakan syntax berikut:"""

data.loc[(data['vol_engine']==0)]

"""dapat dilihat ada beberapa nilai 0 pada vol_engine yang berdampingan dengan mileage. Selanjutnya kita tangani masalah missing value ini dengan menggantinya dengan median karena jika dihapus itu tidak mungkin karena data yang harus dihapus cukup banyak dan itu dapat mempengaruhi dataset. Maka gunakan syntax berikut untuk menangani nya"""

# Drop baris dengan nilai 'vol_engine' = 0
#data = data.loc[(data[['vol_engine']]!=0).all(axis=1)]
 
# Cek ukuran data untuk memastikan baris sudah di-drop

median = data['vol_engine'].median()
data['vol_engine'] = data['vol_engine'].replace(0, median)
data.shape

median = data['mileage'].median()
data['mileage'] = data['mileage'].replace(0, median)
data.shape

data.describe()

"""missing value sudah teratasi dengan menggantinya dengan median

# **Menangani Outliers**
"""

sns.boxplot(x=data['year'])

sns.boxplot(x=data['mileage'])

sns.boxplot(x=data['vol_engine'])

sns.boxplot(x=data['price'])

"""persamaaan untuk mencari batas bawah dan atas adalah sebagai berikut:
* Batas bawah = Q1 - 1.5 * IQR
* Batas atas = Q3 + 1.5 * IQR

data yang nilainya 1.5 QR di atas Q3 atau 1.5 QR di bawah Q1

dapat dilihat dari gambar gambar fitur di atas terdapat banyak outlier, dan untuk menangani hal tersebut maka outlier tersebut dihilangkan dengan menggunakan syntax berikut:
"""

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR=Q3-Q1
data=data[~((data<(Q1-1.5*IQR))|(data>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah kita drop outliers
data.shape

data.describe()

"""sudah tidak terdapat missing value sehingga data sudah layak proses

# **Univariate Analysis**

Analysis dengan teknik univariate EDA. pertama tama :membagi fitur pada dataset menjadi dua bagian, yaitu numerical features dan categorical features.
"""

numerical_features = ['year',	'mileage',	'vol_engine','price']
categorical_features = ['mark',	'model',	'generation_name',	'fuel',	'city',	'province']

"""## Categorical Features

### Fitur mark
"""

feature = categorical_features[0]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""dapat dilihat bahwa mobil yang paling banyak adalah dari pabrikan opel dan yang paling sedikit dari pabrikan chevrolet

### Fitur model
"""

feature = categorical_features[1]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""pada fiitur model ini model yang paling banyak adalah modelastra dan ynag paling sedikit adalah id4

### Fitur generation_name
"""

feature = categorical_features[2]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""pada fitur generation_name yang paling banyak adalah gen-8p-2003-2012

### Fitur fuel
"""

feature = categorical_features[3]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""berdasarkan fitue fuel kebanyakan kendaraan menggunnakan gasoline, diesel, LPG, hybrid, electric, dan yang paling sedikit adalah CNG

## Numerical Features
"""

data.hist(bins=50, figsize=(20,15))
plt.show()

"""disini kita fokuskan analisis kita hanya pada histogram "price"

* Peningkatan harga mobil sebanding dengan penurunan jumlah sampel. Hal ini dapat kita lihat jelas dari histogram "price" yang grafiknya mengalami penurunan seiring dengan semakin banyaknya jumlah sampel (sumbu x).
* Rentang harga diamonds cukup tinggi yaitu dari skala ratusan dolar Amerika hingga sekitar $175000.

# **Exploratory Data Analysis - Multivariate Analysis**

Multivariate EDA menunjukkan hubungan antara dua atau lebih variabel pada data.

## Categorical Features

Pada tahap ini, kita akan mengecek rata-rata harga terhadap masing-masing fitur untuk mengetahui pengaruh fitur kategori terhadap harga
"""

cat_features = data.select_dtypes(include='object').columns.to_list()
 
for col in cat_features:
  sns.catplot(x=col, y="price", kind="bar", dodge=False, height = 4, aspect = 3,  data=data, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

"""* pada fitur mark jenis volvo memiliki harga tertinggi dan yang paling rendah adalah chevrolet
* pada fitur fuel jenis electric memiliki harga paling tinggi diikuti oleh hybrid, dan yang paling rendah adalah LPG

## Numerical Features

Untuk mengamati hubungan antara fitur numerik, kita akan menggunakan fungsi pairplot(). Kita juga akan mengobservasi korelasi antara fitur numerik dengan fitur target menggunakan fungsi corr(). Tidak perlu menunggu lama, mari kita langsung analisis datanya.
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(data, diag_kind = 'kde')

"""pada variabel year terdapat kenaikan sementara itu pada mileage terjadi penurunan, berbeda dengan variabel vol_engine memiliki pola random"""

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr().round(2)
 
# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""vol_engine memiliki pengaruh kecil terhadap price sehingga dapat dihilangkan"""

data.drop(['vol_engine'], inplace=True, axis=1)
data.head()

"""# **Data Preparation**

## Encoding Fitur Kategori

teknik yang digunakan untuk encoding adalah one-hot-encoding. Penggunaan Library scikit-learn untuk mendapatkan fitur baru yang sesuai sehingga dapat mewakili variabel kategori antara lain, 'mark',	'model',	'generation_name',	'fuel',	'city',	'province' proses encoding ini menggunakanfitur get_dummmies
"""

from sklearn.preprocessing import  OneHotEncoder
data = pd.concat([data, pd.get_dummies(data['mark'], prefix='cut')],axis=1)
data = pd.concat([data, pd.get_dummies(data['model'], prefix='color')],axis=1)
data = pd.concat([data, pd.get_dummies(data['generation_name'], prefix='clarity')],axis=1)
data = pd.concat([data, pd.get_dummies(data['fuel'], prefix='clarity')],axis=1)
data = pd.concat([data, pd.get_dummies(data['city'], prefix='clarity')],axis=1)
data = pd.concat([data, pd.get_dummies(data['province'], prefix='clarity')],axis=1)
data.drop(['mark',	'model',	'generation_name',	'fuel',	'city',	'province'], axis=1, inplace=True)
data.head()

"""# **Train-Test-Split**

Membagi dataset menjadi data latih (train) dan data uji (test) hal ini dilakukan sebelum membuat model. Porses ini dilakukan sebelum proses scaling (penyekalaan) bertujuan agar tidak terjadinya kebiciran data (data leakage). Maka proses scaling dilakukan terpisah antara data latih dan data uji.

Pada proyek ini memiliki proporsi pembagian data latih dan data uji sebanyak 90:10 hal ini dikarena dataset yang digunakan berjumlah banyak, dengan random_state = 42.
"""

from sklearn.model_selection import train_test_split
 
X = data.drop(["price"],axis =1)
y = data["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)

"""Untuk mengecek jumlah sampel pada masing-masing bagian, kita gunakan code berikut"""

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""# **Standarisasi**

Proses standardisasi membantu untuk membuat fitur data menjadi bentuk yang lebih mudah diolah oleh algoritma. Pada proyek ini menggunakan teknik StandarScaler dari library scikitlearn.

StandarScaler melakukan proses standardisasi fitur dengan mengurangkan mean kemudian membaginya dengan standar deviasi untuk menggeser distribusi. Penerapan fitur dilakukan pada data latih saja dan untuk data uji akan di lakukan pada tahap evaluasi.
"""

from sklearn.preprocessing import StandardScaler
 
numerical_features = ['year',	'mileage']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""proses standarisasi mengubah nilai rata-rata (mean) menjadi 0 dan nilai standar deviasi menjadi 1. Untuk mengecek nilai mean dan standar deviasi pada setelah proses standarisasi, jalankan kode ini:"""

X_train[numerical_features].describe().round(4)

"""# **Model Development**

Pada proyek ini menggunakan algoritma machine learning KNN, Random Forest, dan Boosting
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""## Random Forest

random forest pada dasarnya adalah versi bagging dari algoritma decision tree. Bagging atau bootstrap aggregating adalah teknik yang melatih model dengan sampel random. Dalam teknik bagging, sejumlah model dilatih dengan teknik sampling with replacement.

mean_squared_error sebagai metrik untuk mengevaluasi performa model
"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
 
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""* n_estimator: jumlah trees (pohon) di forest. Di sini kita set n_estimator=50.
* max_depth: kedalaman atau panjang pohon. Ia merupakan ukuran seberapa banyak pohon dapat membelah (splitting) untuk membagi setiap node ke dalam jumlah pengamatan yang diinginkan.
* random_state: digunakan untuk mengontrol random number generator yang digunakan. 
* n_jobs: jumlah job (pekerjaan) yang digunakan secara paralel. Ia merupakan komponen untuk mengontrol thread atau proses yang berjalan secara paralel. n_jobs=-1 artinya semua proses berjalan secara paralel.

## Boosting

algoritma ini bertujuan untuk meningkatkan performa atau akurasi prediksi. Caranya adalah dengan menggabungkan beberapa model sederhana dan dianggap lemah (weak learners) sehingga membentuk suatu model yang kuat (strong ensemble learner). Algoritma boosting muncul dari gagasan mengenai apakah algoritma yang sederhana seperti linear regression dan decision tree dapat dimodifikasi untuk dapat meningkatkan performa.
Dilihat dari caranya memperbaiki kesalahan pada model sebelumnya, algoritma boosting terdiri dari dua metode:
- Adaptive boosting
- Gradient boosting

Proyek ini menggunakan metode adaptive boosting. Salah satu metode adaptive boosting yang terkenal adalah AdaBoost
"""

from sklearn.ensemble import AdaBoostRegressor
 
boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)                             
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""Berikut merupakan parameter-parameter yang digunakan pada potongan kode di atas.

* learning_rate: bobot yang diterapkan pada setiap regressor di masing-masing proses iterasi boosting.
* random_state: digunakan untuk mengontrol random number generator yang digunakan.

### KNN

KNN bekerja dengan membandingkan jarak satu sampel ke sampel pelatihan lain dengan memilih sejumlah k tetangga terdekat (dengan k adalah sebuah angka positif). Nah, itulah mengapa algoritma ini dinamakan K-nearest neighbor (sejumlah k tetangga terdekat). KNN bisa digunakan untuk kasus klasifikasi dan regresi. Pada modul ini, kita akan menggunakannya untuk kasus regresi.

Kelebihan KNN adalah algoritma yang mudah digunakan dan dipahami, namun memiliki kekurangan jika dihadapkan pada jumlah fitur atau dimensi yang besar.

Pada proyek ini nilai ketetanggaan terdekat(K) adalah 10 dan metode penghitungan jarang nya menggunakan default yaitu minkowski.
"""

from sklearn.neighbors import KNeighborsRegressor

 
knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)
 
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""# Evaluasi

Metrik yang akan kita gunakan pada prediksi ini adalah MSE atau Mean Squared Error yang menghitung jumlah selisih kuadrat rata-rata nilai sebenarnya dengan nilai prediksi.
tapi terlebih dahulu data uji discaling, hal ini dilakukan agar skala antara data latih dan data uji sama.
"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""Selanjutnya, mari kita evaluasi ketiga model kita dengan metrik MSE yang telah dijelaskan di atas."""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])
 
# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
# Panggil mse
mse

"""Saat menghitung nilai Mean Squared Error pada data train dan test, kita membaginya dengan nilai 1e3. Hal ini bertujuan agar nilai mse berada dalam skala yang tidak terlalu besar.

Untuk memudahkan, mari kita plot metrik tersebut dengan bar chart

dapat dilihat model RF memilki error yanng paling sedikit berbeda dengan model boostong
"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Untuk mengujinya, mari kita buat prediksi menggunakan beberapa harga dari data test"""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""dapat dilihat prediksi yang paling mendekati adalah model RF"""